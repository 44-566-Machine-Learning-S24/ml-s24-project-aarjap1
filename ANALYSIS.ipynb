{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0267805-d447-4a1a-84fa-ed17bc7c69c4",
   "metadata": {},
   "source": [
    "## [predicting-parental-educational-level-Project](https://github.com/44-566-Machine-Learning-S24/ml-s24-project-aarjap1/blob/ceeb9e471dc63a9a87fb6595963f56f827501536/Final%20Project.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a98130-4dcb-4ac8-92f9-761dd1a6ef36",
   "metadata": {},
   "source": [
    "## ANALYSIS and Technical Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6abc7d3-70dc-4649-8305-3b05b05f6858",
   "metadata": {},
   "source": [
    "I used the models listed below for my analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6dcb82-300c-4ce4-a430-88d781840663",
   "metadata": {},
   "source": [
    "# 1. Linear Regression\n",
    "\n",
    "To check the notebook for [Linear Regression Click Here](http://localhost:8890/files/Machine%20Learning%20Repos/ml-s24-project-aarjap1/initial_exploration.ipynb?_xsrf=2%7C26852a63%7Cc5f06010968acda4cc4f4638ad709b2c%7C1712674144).\n",
    "\n",
    "**On training set**\n",
    "\n",
    "> R2 score : 0.00030304875264952624\n",
    "\n",
    "The value of R2 is 0.0030 which in training set which is not close to 1 and so it does not show a great fit. This means model is not capturing the true relationship between the total score and parental education level which says that the relation between the two is weak.\n",
    "\n",
    "**On test set**\n",
    "\n",
    "> R2 Score: -0.011741957115891655 and Root Mean Squared Error(MSE): 1.4150789587718846\n",
    "\n",
    "Here in testing set the R2 score is far from 1 which again show it's not a great fit and shows a weak relationship between total score and parental education level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899ce0fd-a2ab-4e40-a4b5-c03be753d042",
   "metadata": {},
   "source": [
    "# 2. Decision Trees\n",
    "\n",
    "To check the notebook for [Decision Trees](http://localhost:8890/files/Machine%20Learning%20Repos/ml-s24-project-aarjap1/classification.ipynb?_xsrf=2%7C26852a63%7Cc5f06010968acda4cc4f4638ad709b2c%7C1712674144).\n",
    "\n",
    "**On training set**\n",
    "\n",
    "> Training Accuracy: 0.894\n",
    "\n",
    "> Training Precision: 0.9046901458182233 \n",
    "\n",
    "The decision tree model did very good on the training set scoring accuray and precison of 0.89 and 0.90 respectively.\n",
    "\n",
    "\n",
    "**On test set**\n",
    "\n",
    "> Test Accuracy: 0.296\n",
    "\n",
    "> Test Precision: 0.33045234708392607\n",
    "\n",
    "But in the test set the model performed poorly with acccuray and precision of only 0.28 and 0.32 respectively. So looking at the metrics value for training and testing set for decision tree classifier we can see the huge gap in the value of metrics which suggests that the model looks to be overfitting. Hence I learned my dataset has a overfitting problem due to the huge gap in metrics score between training and testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b10294-de88-4898-916f-709cf3a4aaa9",
   "metadata": {},
   "source": [
    "# 3. SVM\n",
    "\n",
    "To check the notebook for [SVM](http://localhost:8890/files/Machine%20Learning%20Repos/ml-s24-project-aarjap1/classification.ipynb?_xsrf=2%7C26852a63%7Cc5f06010968acda4cc4f4638ad709b2c%7C1712674144).\n",
    "\n",
    "\n",
    "**On training set**\n",
    "\n",
    "> Training Accuracy (SVM): 0.37\n",
    "\n",
    "> Training Precision (SVM): 0.5415566003616636\n",
    "\n",
    "\n",
    "**On test set**\n",
    "\n",
    "> Test Accuracy (SVM): 0.336\n",
    "\n",
    "> Test Precision (SVM): 0.49453061224489797\n",
    "\n",
    "The SVM model in overall did not do to well but looking at the score over the triaining and test set the scores are consistent which shows that it's not overfitting by too much. Even if it does not do to well since the scores remain consistent it is better overall than the other models in predicting parental level of education using the features race, gender and total score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3a772-5619-40d7-81ee-4a38f475bd93",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Random Forest\n",
    "\n",
    "To check the notebook for [Random Forest](http://localhost:8890/files/Machine%20Learning%20Repos/ml-s24-project-aarjap1/Clustering.ipynb?_xsrf=2%7C26852a63%7Cc5f06010968acda4cc4f4638ad709b2c%7C1712674144).\n",
    " \n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "         1.0       0.86      0.84      0.85       118\n",
    "         2.0       0.88      0.91      0.89       226\n",
    "         3.0       0.96      0.78      0.86        59\n",
    "         4.0       0.85      0.87      0.86       222\n",
    "\n",
    "    accuracy                           0.87       625\n",
    "    \n",
    "   macro avg       0.89      0.85      0.87       625\n",
    "   \n",
    "weighted avg       0.87      0.87      0.87       625\n",
    "\n",
    "The weighted average for the all the performance metrics(precision, recall, f1-score) I got was 87%. The Random forest did the best in predicting the parental level of education after SVM using features race, gender and total score. But I did not pick this as the best model because it might be overfitting due large gaps in precison, recall and f1 scores as you can see above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad461b1-60a3-40eb-802e-216adf8cc2ec",
   "metadata": {},
   "source": [
    "# Challenges Faced\n",
    "\n",
    "1. Challenges I faced was not having proper features in the dataset to predict parental level of education. The dataset had only a limited features which I could test out and from the limited ones only the total score feature which I made was effective. All the other features were not really good in predicting parental level of education. In the future if I had to pick a dataset I would check out the features first before picking my dataset so I can have flexibility in testing out the features in predicting my dependent variable\n",
    "\n",
    "2. Required a lot of cleaning as most of the features were non-numeric and numeric features were limited to the dataset\n",
    "\n",
    "3. Not really a challenge but having to many options for model and deciding which to pick for my dataset was also a trouble. Like for example with SVM kernels I had too many options for which kernel to go with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23384c2d-e1d8-4e13-b7f5-685f40d03b14",
   "metadata": {},
   "source": [
    "# Improvements and Future Prospects\n",
    "\n",
    "1. In the future I would definitely pick a dataset which has a lot of features. Also the feature I am planning to predict should have a better co-relation with other features when picking out the dataset. \n",
    "\n",
    "2. Maybe use a different split size which might be useful in order to make a better prediction.\n",
    "\n",
    "3. If I were to have more features for this dataset I would ask the person who made the dataset to add features like Parent's job, Student's previous semster GPA's, study hours and so on which could play a huge factor in prediciting the outcome.\n",
    "\n",
    "4. Try out more models so I can see how other model does with the dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
